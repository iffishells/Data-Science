{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Data-Analysis-Basic-Analysis.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNh26BATLK6iccUVYwWSklw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iffishells/Data-Science/blob/main/Data_Analysis_Basic_Analysis_KeyWordExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4xSiZOhiR8j"
      },
      "source": [
        "**Data Analysis** - Basic **Analysis** \n",
        "\n",
        "Text analysis is a machine learning technique that allows companies to automatically understand text data, such as tweets, emails, support tickets, product reviews, and survey responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXMPsWMyiRnP"
      },
      "source": [
        "text = '''The culmination of nearly four years of continuous work, \\n The Encyclopedia of Systemic Neuro-Linguistic Programming and NLP New Coding written by Robert Dilts \\n Judith DeLozier provides a fascinating and comprehensive guide to the field of NLP.\\n\n",
        "This beautifully presented, hardbound 2-volume set includes \\n\n",
        "descriptions of fundamental techniques and models \\n\n",
        "definitions of core NLP concepts \\n\n",
        "biographical entries for key developers and contributors \\n\n",
        "historical influences that were sources of key NLP ideas and concepts \\n\n",
        "exemplars who served as the initial models for NLP \\n\n",
        "fields and disciplines. related to NLP \\n\n",
        "extensive illustrations, tables, charts, worksheets, and questionnaires \\n\n",
        "biography of reference texts and related readings \\n\n",
        " comprehensive index with cross-references \\n\n",
        "\n",
        "A distinguishing feature of this Encyclopedia . is its emphasis on knowledge that can. be pragmatically applied to your life experience. \\nThis is expressed by the large number of exercises and activities.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqcZ7RnwhtSp"
      },
      "source": [
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CosbQRKgpcwp"
      },
      "source": [
        "docs = text.split('\\n')\n",
        "print(\"Lenght of docs : \",len(docs) )\n",
        "docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ1YoN2Xph58"
      },
      "source": [
        "## find the number of sentance\n",
        "# \" . \" point to end the sentance \n",
        "noSent = 0\n",
        "for d in docs:\n",
        "    # print(\"One sentance  : \",d)\n",
        "    for char in d:\n",
        "        # print(\"Char in Sentance : \",char)\n",
        "        if char == \".\":\n",
        "            noSent+=1\n",
        "\n",
        "noSent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GZTKfTT0Yp0"
      },
      "source": [
        "# Other way\n",
        "noSent = 0\n",
        "\n",
        "for d in docs:\n",
        "    index = 0\n",
        "    while index !=-1:\n",
        "\n",
        "        # syntax\n",
        "        # find(What you find to search , from , where) return index\n",
        "        index = d.find('.',index+1,len(d))\n",
        "        # print(index)\n",
        "        noSent +=1\n",
        "    noSent-=1\n",
        "\n",
        "noSent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwOsCyal1pHQ"
      },
      "source": [
        "***find the Number of words in the Documnets***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk5lF53H1V6i"
      },
      "source": [
        "noWords = 0\n",
        "\n",
        "for d in docs:\n",
        "    words = d.split(\" \")\n",
        "    print(words)\n",
        "    noWords += len(words)\n",
        "\n",
        "noWords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL6542vg2sKZ"
      },
      "source": [
        "## *Building the Dictionary*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipBDYU1y2K93"
      },
      "source": [
        "vacub = []\n",
        "\n",
        "for d in docs:\n",
        "\n",
        "    words = d.split(\" \")\n",
        "\n",
        "    for word in words:\n",
        "        if word not in vacub:\n",
        "            vacub.append(word)\n",
        "\n",
        "vacub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioEJr67L3S7V"
      },
      "source": [
        "## *VacubSize*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGW1Ybeg3LjF"
      },
      "source": [
        "vacubSize = len(vacub)\n",
        "vacubSize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GhCqnrf3sMs"
      },
      "source": [
        "# *Sentance per Document*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8lpk9v_3fLn"
      },
      "source": [
        "print(\"Len of docs : \",len(docs))\n",
        "print(\"No of sentance : \", noSent)\n",
        "noSent / len(docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll-3kP1E31rP"
      },
      "source": [
        "# *words per sentance*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkkvbH4S3zpl"
      },
      "source": [
        "noWords / noSent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1PAM9vs394Y"
      },
      "source": [
        "## *Word per Document*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vOhboom37Tm"
      },
      "source": [
        "noWords / len(docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYRoJAyd4L-P"
      },
      "source": [
        "# *Words per Unique words*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPCB7KWl4FKE"
      },
      "source": [
        "noWords / vacubSize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ij2wxD-4WhY"
      },
      "source": [
        "# *Char per Words*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1Cx21O94R_V"
      },
      "source": [
        "nochar = 0\n",
        "for d in docs:\n",
        "    for char in d:\n",
        "        nochar +=1\n",
        "\n",
        "nochar\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6mclINd5B6E"
      },
      "source": [
        "nochar / noWords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjnFKvnQ5QKW"
      },
      "source": [
        "# Another option\n",
        "noAllchar = 0\n",
        "for d in docs:\n",
        "    noAllchar +=len(d)\n",
        "\n",
        "noAllchar / noWords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkrMr9E_B6fU"
      },
      "source": [
        "# ***Most Frequent Word and Most inFrequent words in Documents***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1jvkrIy5_V2"
      },
      "source": [
        "wordFreq = {}\n",
        "for d in docs:\n",
        "    words = d.split(\" \")\n",
        "    # print(words)\n",
        "    for word in words:\n",
        "        if word in wordFreq.keys():\n",
        "            wordFreq[word] +=1\n",
        "        else:\n",
        "            wordFreq[word] = 1\n",
        "# wordFreq[\"Coding\"]  # return number of freq\n",
        "maxFreq = max(wordFreq.values())\n",
        "print(\"Max Frequency : \",maxFreq)\n",
        "minFreq = min(wordFreq.values())\n",
        "print(\"Min Frequecy : \",minFreq)\n",
        "\n",
        "\n",
        "minFreqWords = []\n",
        "maxFreqWords = []\n",
        "\n",
        "for key in wordFreq.keys():\n",
        "    if wordFreq[key]==minFreq:\n",
        "        minFreqWords.append(key)\n",
        "    elif wordFreq[key]==maxFreq:\n",
        "        maxFreqWords.append(key)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRPMc4jlEmVx"
      },
      "source": [
        "### ***Min Frequency words***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JD1MFLhCsPo"
      },
      "source": [
        "minFreq ,minFreqWords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBG22r2xFHBu"
      },
      "source": [
        "# ***Max Frequency Words***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBOMzQr1EsAA"
      },
      "source": [
        "maxFreq ,maxFreqWords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tE3z0sML_y4"
      },
      "source": [
        "# ***Finding The longest Token and shortest Token in the Documents***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w_MT-anFUAD"
      },
      "source": [
        "!pip install stop-words\n",
        "\n",
        "from stop_words import get_stop_words\n",
        "\n",
        "stop_words = get_stop_words('en')\n",
        "stop_words = get_stop_words('english')\n",
        "stop_words.append('.')\n",
        "stop_words.append('A')\n",
        "stop_words.append('') \n",
        "\n",
        "print(stop_words)\n",
        "# from stop_words import safe_get_stop_words\n",
        "\n",
        "# stop_words = safe_get_stop_words('unsupported language')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5GqhNDuMI7w"
      },
      "source": [
        "minTokenSize = len(vacub[0])\n",
        "minTokenWords = []\n",
        "\n",
        "maxTokenSize = len(vacub[0])\n",
        "maxTokenWords = []\n",
        "\n",
        "for token in vacub:\n",
        "    if token not in stop_words:\n",
        "        print(token)\n",
        "        if len(token) > maxTokenSize:\n",
        "            maxTokenSize =  len(token)\n",
        "        elif len(token) < minTokenSize:\n",
        "            minTokenSize = len(token)\n",
        "\n",
        "print(\"Min Token Size : \", minTokenSize)\n",
        "print(\"Max Token Size : \",maxTokenSize)\n",
        "for token in vacub:\n",
        "    if token not in stop_words:\n",
        "        if len(token) == maxTokenSize:\n",
        "            maxTokenWords.append(token)\n",
        "        elif len(token) == minTokenSize:\n",
        "            minTokenWords.append(token)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt4ym7ULPjGf"
      },
      "source": [
        "# Shorest **words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMz2sfdUMgSS"
      },
      "source": [
        "minTokenSize , minTokenWords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl2Vu9_rPtKY"
      },
      "source": [
        "# Longest **Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_jvSaayPzBp"
      },
      "source": [
        "maxTokenSize , maxTokenWords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDQivSGBQP2b"
      },
      "source": [
        "## **Task**\n",
        "\n",
        "---\n",
        "1.   *find the Shortest Documents by Number of words*\n",
        "2.   *Find the Longest Documents by number of Characters*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpb3SfHtgAs6"
      },
      "source": [
        "# ***Visual Representation of Words and their Frequency***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR9osrpXPpIY"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.plot( list(wordFreq.keys()),\n",
        "        list(wordFreq.values()))\n",
        "plt.title(\"Big of Picture of Words Frequency\")\n",
        "plt.xlabel(\"Tokens\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxKQUpaNjcQp"
      },
      "source": [
        "## **Other Ways**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbIN8HB0gNrs"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.plot( [i for i in range(len(vacub))],\n",
        "        list(wordFreq.values()))\n",
        "plt.title(\"Big of Picture of Words Frequency\")\n",
        "plt.xlabel(\"Tokens\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIecShb3orz1"
      },
      "source": [
        "# **Keyword Extraction**\n",
        "\n",
        "\n",
        "---\n",
        "Normaly When we are getting the data from the real life then we can't putt all the data into the Machin learning model.Yet what i have Done is only Analysis of Our data .What type of our Data about their frequency and about Growth rate etc.Now we Reached extracting helpfull Data from the Whole data are called Keyword.When we are attracting the keyword from the token  we left the min freq date because they are usaually stopword or missing word and we also left the max freq data because they are usaually stopword .lets come to the implementation\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqvsCtSPiLLC"
      },
      "source": [
        "# words =  list(wordFreq.keys())\n",
        "# print(words)\n",
        "wordFreq = list(wordFreq.values())\n",
        "print(\"Frequency \\n : \",wordFreq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUtSJVMwp-xw"
      },
      "source": [
        " print(list(wordFreq.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyAIm7k8rCH7"
      },
      "source": [
        "print(wordFreq.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nvbLoScrt2V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}